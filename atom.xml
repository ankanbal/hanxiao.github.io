<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Han Xiao Tech Blog - Neural Search, AI, Engineering</title>
  <icon>https://www.gravatar.com/avatar/4c2f23c2a19e55a6682ad6b3b7216ccf</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://hanxiao.io/"/>
  <updated>2020-09-21T16:51:02.000Z</updated>
  <id>https://hanxiao.io/</id>
  
  <author>
    <name>Han Xiao</name>
    <email>artex.xh@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Numpy Tricks and A Strong Baseline for Vector Index</title>
    <link href="https://hanxiao.io/2020/09/21/Numpy-Tricks-and-A-Strong-Baseline-for-Vector-Index/"/>
    <id>https://hanxiao.io/2020/09/21/Numpy-Tricks-and-A-Strong-Baseline-for-Vector-Index/</id>
    <published>2020-09-21T10:07:20.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;I like simple things and not dependencies. When it comes to architecting &lt;a href=&quot;https://github.com/jina-ai/jina&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Jina&lt;/a&gt;, I keep its core dependencies as simple as &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;pyzmq&lt;/code&gt;, &lt;code&gt;protobuf&lt;/code&gt;, and &lt;code&gt;grpcio&lt;/code&gt; while still delivering full-stack functionalities. This allows Jina developers &amp;amp; users to bootstrap an end-to-end neural search system without extra packages quickly.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>New Features in Jina &lt;code&gt;v0.5&lt;/code&gt; You Should Know About</title>
    <link href="https://hanxiao.io/2020/08/28/What-s-New-in-Jina-v0-5/"/>
    <id>https://hanxiao.io/2020/08/28/What-s-New-in-Jina-v0-5/</id>
    <published>2020-08-28T10:00:02.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;p&gt;Today we are excited to release Jina &lt;code&gt;v0.5.0&lt;/code&gt;. &lt;a href=&quot;https://github.com/jina-ai/jina/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Jina is an easier way for enterprises and developers to build neural search in the cloud&lt;/a&gt;. The new version is a great accomplishment made by our engineers and the open-source community: it contains 700 new commits covering 400 files and &lt;strong&gt;7000 lines&lt;/strong&gt; of code since &lt;code&gt;v0.4.0&lt;/code&gt;. The new release also introduces breaking changes and advanced features such as recursive document representation, query language driver, and hub rebuild.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Engineering All Hands in Public</title>
    <link href="https://hanxiao.io/2020/08/06/Engineering-All-Hands-in-Public/"/>
    <id>https://hanxiao.io/2020/08/06/Engineering-All-Hands-in-Public/</id>
    <published>2020-08-06T13:02:36.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;p&gt;When I worked at Tencent Opensource Program Office and the Linux Foundation, I realized the importance of tech culture to a company and how hard it is to adapt to a new tech culture once the company is big enough: frontline engineers need to work with the right tech stack and be inclusive to the community; mid-level managers need to include open-source metrics in their KPI/OKR; high-level executive need to see the strategic value of being culturally-advanced.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Layer of Abstraction When Building &quot;Tensorflow&quot; for Search</title>
    <link href="https://hanxiao.io/2020/08/02/Layer-of-Abstraction-when-Building-Tensorflow-for-Search/"/>
    <id>https://hanxiao.io/2020/08/02/Layer-of-Abstraction-when-Building-Tensorflow-for-Search/</id>
    <published>2020-08-02T08:04:54.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;p&gt;Since Feb. 2020, I started a new venture called &lt;a href=&quot;https://www.linkedin.com/company/jinaai&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Jina AI&lt;/a&gt;. Our mission is to build an open-source neural search ecosystem for businesses and developers, allowing everyone to search for information in all kinds of data with high availability and scalability.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>I Left Tencent AI and Raised $2M to Build the Next Neural Search Company in Open Source</title>
    <link href="https://hanxiao.io/2020/02/18/I-Quitted-Tencent-AI-and-Raised-M-to-Build-the-Next-Neural-Search-Company-in-Opensource/"/>
    <id>https://hanxiao.io/2020/02/18/I-Quitted-Tencent-AI-and-Raised-M-to-Build-the-Next-Neural-Search-Company-in-Opensource/</id>
    <published>2020-02-18T22:02:39.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;TLDR&quot;&gt;&lt;a href=&quot;#TLDR&quot; class=&quot;headerlink&quot; title=&quot;TLDR;&quot;&gt;&lt;/a&gt;TLDR;&lt;/h2&gt;&lt;p&gt;Two years after moving back to China and joined Tencent AI Lab, it is time for me to take the next step and move forward. Only this time, I’m starting &lt;em&gt;my own business&lt;/em&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The 2M USD seed funding is ready for our dreams to take off –– to build the next neural search company.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Video Semantic Search in Large Scale using GNES and Tensorflow 2.0</title>
    <link href="https://hanxiao.io/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/"/>
    <id>https://hanxiao.io/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/</id>
    <published>2019-11-22T15:10:38.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;p&gt;Many people may know me from &lt;a href=&quot;https://github.com/hanxiao/bert-as-service&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;code&gt;bert-as-service&lt;/code&gt;&lt;/a&gt; (and of course from &lt;a href=&quot;https://github.com/zalandoresearch/fashion-mnist&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Fashion-MNIST&lt;/a&gt; &lt;code&gt;&amp;lt;/bragging&amp;gt;&lt;/code&gt;). So when they first heard about my new project &lt;a href=&quot;/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/&quot; title=&quot;GNES: Generic Neural Elastic Search&quot;&gt;GNES: Generic Neural Elastic Search&lt;/a&gt;, people naturally think that I’m building a semantic text search solution. But actually, GNES has a more &lt;em&gt;ambitious&lt;/em&gt; goal to become the next-generation semantic search engine for all content forms, including text, image, video and audio.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>A Better Practice for Managing Many &lt;code&gt;extras_require&lt;/code&gt; Dependencies in Python</title>
    <link href="https://hanxiao.io/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/"/>
    <id>https://hanxiao.io/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/</id>
    <published>2019-11-07T16:16:50.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;p&gt;One problem I was facing when building &lt;a href=&quot;https://github.com/gnes-ai/gnes/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GNES: Generic Neural Elastic Search&lt;/a&gt; is how to effectively control the dependencies in Python. The main dependencies of GNES is very simple: &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;grpcio&lt;/code&gt;, &lt;code&gt;pyzmq&lt;/code&gt; and a YAML parser. But this only runs a vanilla GNES with no fancy deep learning models nor preprocessors.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>站在世界舞台上看开源 - 我在国际AI开源基金会当董事</title>
    <link href="https://hanxiao.io/2019/11/05/%E7%AB%99%E5%9C%A8%E4%B8%96%E7%95%8C%E8%88%9E%E5%8F%B0%E4%B8%8A%E7%9C%8B%E5%BC%80%E6%BA%90-%E6%88%91%E5%9C%A8%E5%9B%BD%E9%99%85AI%E5%BC%80%E6%BA%90%E5%9F%BA%E9%87%91%E4%BC%9A%E5%BD%93%E8%91%A3%E4%BA%8B/"/>
    <id>https://hanxiao.io/2019/11/05/站在世界舞台上看开源-我在国际AI开源基金会当董事/</id>
    <published>2019-11-05T15:00:24.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;div class=&quot;remind&quot;&gt;&lt;br&gt;This article is originally posted at &lt;a href=&quot;https://www.qbitai.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;量子位/Qbit&lt;/a&gt; on Oct. 13, 2019, a Chinese tech media focusing AI. I shared my experience of serving as a board member in the &lt;a href=&quot;https://lfai.foundation/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Linux Foundation AI&lt;/a&gt;, how I enabled the collaboration and the creation of new opportunities for all the members of the community and pointed out some challenges of Chinese OSS going aboard.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines</title>
    <link href="https://hanxiao.io/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/"/>
    <id>https://hanxiao.io/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/</id>
    <published>2019-10-18T13:51:04.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;div class=&quot;remind&quot;&gt;&lt;br&gt;For those who don’t know about &lt;a href=&quot;https://github.com/gnes-ai/gnes&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GNES&lt;/a&gt;. GNES &lt;code&gt;[jee-nes]&lt;/code&gt; is Generic Neural Elastic Search, a cloud-native semantic search system based on deep neural network. GNES enables large-scale index and semantic search for text-to-text, image-to-image, video-to-video and any-to-any content form. More information can be found in &lt;a href=&quot;https://github.com/gnes-ai/gnes&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;our Github repository&lt;/a&gt;.&lt;br&gt;&lt;/div&gt;&lt;p&gt;Since this March, &lt;a href=&quot;https://github.com/gnes-ai/gnes&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GNES&lt;/a&gt; has evolved over &lt;a href=&quot;https://github.com/gnes-ai/gnes/blob/master/CHANGELOG.md&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;46 versions&lt;/a&gt; in the last six months. In the most recent release &lt;code&gt;v0.0.46&lt;/code&gt;, we publish a new set of API called GNES Flow. It offers a &lt;em&gt;pythonic&lt;/em&gt; way for users to construct pipelines in GNES with clean, readable idioms.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Generic Neural Elastic Search: From &lt;code&gt;bert-as-service&lt;/code&gt; and Go Way Beyond</title>
    <link href="https://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/"/>
    <id>https://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/</id>
    <published>2019-07-29T12:38:44.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;p&gt;Since Jan. 2019, I have started leading a team at Tencent AI Lab and working on a new system &lt;strong&gt;GNES (Generic Neural Elastic Search)&lt;/strong&gt;. GNES is an open-source cloud-native semantic search solution based on deep neural network.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Serving Google BERT in Production using Tensorflow and ZeroMQ</title>
    <link href="https://hanxiao.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/"/>
    <id>https://hanxiao.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/</id>
    <published>2019-01-02T14:35:03.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;div class=&quot;quiz&quot;&gt;&lt;br&gt;This is a post explaining the design philosphy behind my open-source project &lt;a href=&quot;https://github.com/hanxiao/bert-as-service&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;code&gt;bert-as-service&lt;/code&gt;&lt;/a&gt;, a highly-scalable sentence encoding service based on Google BERT and ZeroMQ. It allows one to map a variable-length sentence to a fixed-length vector. In case you haven’t checked it out yet, &lt;a href=&quot;https://github.com/hanxiao/bert-as-service&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/hanxiao/bert-as-service&lt;/a&gt;&lt;br&gt;&lt;/div&gt;&lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;p&gt;When we look back at 2018, one of the biggest news in the world of ML and NLP is Google’s &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bidirectional Encoder Representations from Transformers&lt;/a&gt;, aka BERT. BERT is a method of pre-training language representations which achieves not only state-of-the-art but &lt;em&gt;record-breaking&lt;/em&gt; results on a wide array of NLP tasks, such as &lt;a href=&quot;/2018/04/21/Teach-Machine-to-Comprehend-Text-and-Answer-Question-with-Tensorflow/&quot; title=&quot;machine reading comprehension&quot;&gt;machine reading comprehension&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;To my team at Tencent AI Lab, BERT is particularly interesting as it provides a novel way to represent the semantic of text using real-valued fixed-length vectors. For many real-world NLP/AI applications that we are working on, an effective vector representation is the cornerstone. For example in the neural information retrieval, query and document need to be mapped to the same vector space, so that their relatedness can be computed using a metric defined in this space, e.g. Euclidean or cosine distance. The effectiveness of the representation directly determines the quality of the search.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Fashion-MNIST: Year In Review</title>
    <link href="https://hanxiao.io/2018/09/28/Fashion-MNIST-Year-In-Review/"/>
    <id>https://hanxiao.io/2018/09/28/Fashion-MNIST-Year-In-Review/</id>
    <published>2018-09-28T06:01:41.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;It’s been one year since I released &lt;a href=&quot;https://github.com/zalandoresearch/fashion-mnist&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;the Fashion-MNIST dataset&lt;/a&gt; in Aug. 2017. As I wrote in the &lt;a href=&quot;https://github.com/zalandoresearch/fashion-mnist&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;code&gt;README.md&lt;/code&gt;&lt;/a&gt;, Fashion-MNIST is intended to serve as a &lt;em&gt;drop-in replacement&lt;/em&gt; for the original MNIST dataset, helping people to benchmark and understand machine learning algorithms. Over a year, I have seen a great deal of trends and developments in the machine learning
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Reading Comprehension Part II: Learning to Ask &amp; Answer</title>
    <link href="https://hanxiao.io/2018/09/09/Dual-Ask-Answer-Network-for-Machine-Reading-Comprehension/"/>
    <id>https://hanxiao.io/2018/09/09/Dual-Ask-Answer-Network-for-Machine-Reading-Comprehension/</id>
    <published>2018-09-09T12:11:58.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Recap&quot;&gt;&lt;a href=&quot;#Recap&quot; class=&quot;headerlink&quot; title=&quot;Recap&quot;&gt;&lt;/a&gt;Recap&lt;/h2&gt;&lt;p&gt;In &lt;a href=&quot;/2018/04/21/Teach-Machine-to-Comprehend-Text-and-Answer-Question-with-Tensorflow/&quot; title=&quot;the last post of this series&quot;&gt;the last post of this series&lt;/a&gt;, I have introduced the task of machine reading comprehension (MRC) and presented a simple neural architecture for tackling such task. In fact, this architecture can be found in many state-of-the-art MRC models, e.g. BiDAF, S-Net, R-Net, match-LSTM, ReasonNet, Document Reader, Reinforced Mnemonic Reader, FusionNet and QANet.&lt;/p&gt;&lt;p&gt;I also pointed out an assumption made in this architecture: the answer is always a continuous span of a given passage. Under this assumption, an answer can be simplified as a pair of two integers, representing its start and end position
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>德国人工智能对比中国人工智能：大学、产业及规划</title>
    <link href="https://hanxiao.io/2018/08/25/%E8%B0%88%E6%83%85%E8%AF%B4-AI-%E8%AE%BF%E5%BE%B7%E4%B8%AD%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8D%8F%E4%BC%9A%E4%B8%BB%E5%B8%AD%E8%82%96%E6%B6%B5%E5%8D%9A%E5%A3%AB/"/>
    <id>https://hanxiao.io/2018/08/25/谈情说-AI-访德中人工智能协会主席肖涵博士/</id>
    <published>2018-08-25T19:32:57.000Z</published>
    <updated>2020-09-21T16:51:03.000Z</updated>
    
    <summary type="html">
    
      &lt;div class=&quot;remind&quot;&gt;&lt;br&gt;This is an interview I received from &lt;a href=&quot;https://www.dialogde.cn/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;对话德国/DialogDE&lt;/a&gt;, a Chinese media focusing on culture exchange between Germany and China. The article was originally titled as “谈情说-AI-访德中人工智能协会主席肖涵博士” and published on Aug. 25, 2018. In this article, I shared my nine-year study &amp;amp; work experience on AI in Germany; compared German AI and Chinese AI industry &amp;amp; development; interpreted the tech and academic culture of two countries; and explained my mission at German-Chinese Association for Artificial Intelligence.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>4 Sequence Encoding Blocks You Must Know Besides RNN/LSTM in Tensorflow</title>
    <link href="https://hanxiao.io/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/"/>
    <id>https://hanxiao.io/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/</id>
    <published>2018-06-24T16:19:00.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;p&gt;Understanding human language is a challenging task for computers, as they were originally designed for crunching numbers. To let computers comprehend text as humans do, one needs to encode the complexities and nuances of natural language into numbers. For many years, &lt;a href=&quot;https://en.wikipedia.org/wiki/Recurrent_neural_network&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;recurrent neural networks (RNN)&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Long_short-term_memory&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;long-short term memory (LSTM)&lt;/a&gt; was the way to solve sequence encoding problem. They have indeed accomplished amazing results in many applications, e.g. machine translation and voice recognition. As for me, they were the first solution that comes to my mind when facing an NLP problem. You can find my previous posts about RNN/LSTM in &lt;a href=&quot;/2018/04/21/Teach-Machine-to-Comprehend-Text-and-Answer-Question-with-Tensorflow/&quot; title=&quot;here&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;/2018/01/10/Build-Cross-Lingual-End-to-End-Product-Search-using-Tensorflow/&quot; title=&quot;here&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/&quot; title=&quot;and here&quot;&gt;and here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Now at Tencent AI Lab, however, I’m &lt;em&gt;sunsetting&lt;/em&gt; RNN/LSTM in my team. Why? Because they are &lt;strong&gt;computationally expensive&lt;/strong&gt;, aka slow! The recursive nature
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Teach Machine to Comprehend Text and Answer Question with Tensorflow - Part I</title>
    <link href="https://hanxiao.io/2018/04/21/Teach-Machine-to-Comprehend-Text-and-Answer-Question-with-Tensorflow/"/>
    <id>https://hanxiao.io/2018/04/21/Teach-Machine-to-Comprehend-Text-and-Answer-Question-with-Tensorflow/</id>
    <published>2018-04-21T08:03:42.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;div class=&quot;tip&quot;&gt;&lt;br&gt;The &lt;a href=&quot;/2018/09/09/Dual-Ask-Answer-Network-for-Machine-Reading-Comprehension/&quot; title=&quot;part II of this series&quot;&gt;part II of this series&lt;/a&gt; is avaliable now, in which I present a unified model for asking &lt;em&gt;and&lt;/em&gt; answering!&lt;br&gt;&lt;/div&gt;&lt;p&gt;Reading comprehension is one of the fundamental skills for human, which one must learn systematically since the elementary school. Do you still remember how the worksheet of your reading class looks like? It usually consists of an article and few questions about its content. To answer these questions, you need to first gather information by collecting answer-related sentences from the article. Sometimes you can directly copy those original sentences from the article as the final answer. This is a trivial “gut question”, and every student likes it. Unfortunately (for students), quite often you need to summarize, assert, infer, refine those evidences and finally write the answer in your own words. Drawing inferences about the writer’s intention is especially hard. Back in
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Building Cross-Lingual End-to-End Product Search with Tensorflow</title>
    <link href="https://hanxiao.io/2018/01/10/Build-Cross-Lingual-End-to-End-Product-Search-using-Tensorflow/"/>
    <id>https://hanxiao.io/2018/01/10/Build-Cross-Lingual-End-to-End-Product-Search-using-Tensorflow/</id>
    <published>2018-01-10T15:24:06.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;p&gt;Product search is one of the key components in an online retail store. Essentially, you need a system that matches a text query with a set of products in your store. A good product search can understand user’s query in any language, retrieve as many relevant products as possible, and finally present the result as a list, in which the preferred products should be at the top, and the irrelevant products should be at the bottom.&lt;/p&gt;&lt;p&gt;Unlike text retrieval (e.g. Google web search), products are structured data. A product is often described by a list of key-value pairs, a set of pictures and some free text. In the developers’ world,
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Use &lt;code&gt;HParams&lt;/code&gt; and YAML to Better Manage Hyperparameters in Tensorflow</title>
    <link href="https://hanxiao.io/2017/12/21/Use-HParams-and-YAML-to-Better-Manage-Hyperparameters-in-Tensorflow/"/>
    <id>https://hanxiao.io/2017/12/21/Use-HParams-and-YAML-to-Better-Manage-Hyperparameters-in-Tensorflow/</id>
    <published>2017-12-21T13:02:52.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;p&gt;Building a machine learning system is an exploration-exploitation process. First, you explore some models, network architectures to check which one better fits the given problem. Once you have an idea, you concentrate on a particular model and exploit it by (manually/automatically) tuning its hyperparameters. Finally, the winner model with the best parameters will be deployed online to serve real customers. The whole process involves &lt;em&gt;repetitively&lt;/em&gt; switching between different contexts and environments: e.g. training, validating and testing; local and remote; CPU, GPU, multi-GPU. As a consequence, how to effectively manage the
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Optimizing Contrastive/Rank/Triplet Loss in Tensorflow for Neural Information Retrieval</title>
    <link href="https://hanxiao.io/2017/11/08/Optimizing-Contrastive-Rank-Triplet-Loss-in-Tensorflow-for-Neural/"/>
    <id>https://hanxiao.io/2017/11/08/Optimizing-Contrastive-Rank-Triplet-Loss-in-Tensorflow-for-Neural/</id>
    <published>2017-11-08T12:38:12.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;p&gt;Recently, my team is applying deep neural networks to improve the search experience of customers. Researchers often call this type of application &lt;em&gt;neural information retrieval&lt;/em&gt;. The input to the model is a full-text query and a set of documents. A search query typically contains a few terms, while a document, depending on the scenario, may
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Fashion-MNIST: a Drop-In Replacement of MNIST for Benchmarking Machine Learning Algorithms</title>
    <link href="https://hanxiao.io/2017/08/26/Fashion-MNIST-a-Drop-In-Replacement-of-MNIST-for-Benchmarking-Machine-Learning-Algorithms/"/>
    <id>https://hanxiao.io/2017/08/26/Fashion-MNIST-a-Drop-In-Replacement-of-MNIST-for-Benchmarking-Machine-Learning-Algorithms/</id>
    <published>2017-08-26T10:28:50.000Z</published>
    <updated>2020-09-21T16:51:02.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;TL-DR&quot;&gt;&lt;a href=&quot;#TL-DR&quot; class=&quot;headerlink&quot; title=&quot;TL;DR&quot;&gt;&lt;/a&gt;TL;DR&lt;/h2&gt;&lt;p&gt;The dataset is here: &lt;a href=&quot;https://github.com/zalandoresearch/fashion-mnist&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/zalandoresearch/fashion-mnist&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://research.zalando.com/welcome/team/han-xiao/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;I&lt;/a&gt; and my colleague &lt;a href=&quot;https://research.zalando.com/welcome/team/kashif-rasul/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kashif Rasul&lt;/a&gt; create this image dataset as &lt;strong&gt;a drop-in replacement of MNIST&lt;/strong&gt; for benchmarking machine learning algorithms. The dataset is published under MIT License.&lt;/p&gt;&lt;p&gt;We would appreciate references to the following paper if you use this dataset in publications:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;a href=&quot;arxiv.pdf&quot;&gt;Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms.&lt;/a&gt; Han Xiao, Kashif Rasul, Roland Vollgraf. arXiv: cs.LG/1708.07747&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
