<!DOCTYPE html><html lang="en"><style>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,header,main{display:block}a{background-color:transparent}h1{font-size:2em;margin:.67em 0}img{border:0}body,html{width:100%;height:100%}body{margin:0;color:#34495e;font-size:18px;line-height:1.6;background-color:#fff;font-family:sourcesanspro,'Helvetica Neue',Arial,sans-serif}ul.nav{margin:0;padding:0;list-style-type:none}ul{margin:1rem 0}a{color:#2c3e50;text-decoration:none}.flag-icon{height:25px;width:25px;display:inline;border-radius:50%;vertical-align:sub}.icon_item{padding-left:5px!important;padding-right:5px!important}header{min-height:60px}header .logo-link{float:left}header .nav{float:right;left:80px}header .logo-link img{height:60px}header .nav-list-item{display:inline-block;padding:19px 10px}header .nav-list-item a{line-height:1.4}@media screen and (max-width:900px){header .nav-list-item a{font-size:15px}}@media screen and (min-width:900px){header .nav-list-item a{font-size:18px}}.post{padding-top:1em}.post-block .post-title{margin:.65em 0;color:#2c3e50;font-size:1.5em}.post-block .post-info{color:#7f8c8d}.post-block .post-info .read-time{text-align:right}.post-content h2,.post-content h4{position:relative;margin:1em 0}.post-content h2 :before,.post-content h4 :before{content:"#";color:#42b983;position:absolute;left:-.7em;top:-4px;font-size:1.2em;font-weight:700}.post-content h4 :before{content:">"}.post-content h2{font-size:22px}.post-content h4{font-size:18px}.post-content a{color:#42b983;word-break:break-all}main.container{margin:2em 10px}@media screen and (min-width:900px){.wrap{width:900px;margin:0 auto}header{padding:20px 60px}}@media screen and (max-width:900px){.wrap{width:100%}header{min-height:50px;padding:2px 2px;position:fixed;z-index:10000;border-radius:15px;left:50%;-webkit-transform:translateX(-50%);transform:translateX(-50%);width:-webkit-fit-content;width:-moz-fit-content;width:fit-content}header a.logo-link,header ul.nav.nav-list{float:none;display:inline;text-align:center}header li.nav-list-item{padding:10px 5px}header .logo-link img{height:20px;vertical-align:sub}header .flag-icon{height:20px;width:20px}header{background-color:rgba(255,255,255,.9)}@supports ((-webkit-backdrop-filter:blur(2em)) or (backdrop-filter:blur(2em))){header{background-color:rgba(255,255,255,.3);-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px)}}main.container{padding-top:2em}main.container{margin:0 20px}.post-content h2,.post-content h4{max-width:300px;left:15px}}@font-face{font-family:sourcesanspro;src:url(/font/sourcesanspro.woff2) format("woff2"),url(/font/sourcesanspro.woff) format("woff");font-weight:400;font-style:normal}</style><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title>Numpy Tricks and A Strong Baseline for Vector Index ¬∑ Han Xiao Tech Blog - Neural Search, AI, Engineering</title><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@hxiao"><meta name="twitter:creator" content="@hxiao"><meta name="description" content="I like simple things and not dependencies. When it comes to architecting Jina, I keep its core dependencies as simple as numpy, pyzmq, protobuf, and g ... ¬∑ Han Xiao"><meta property="og:title" content="Numpy Tricks and A Strong Baseline for Vector Index ¬∑ Han Xiao Tech Blog - Neural Search, AI, Engineering"><meta property="og:description" content="I like simple things and not dependencies. When it comes to architecting Jina, I keep its core dependencies as simple as numpy, pyzmq, protobuf, and g ... ¬∑ Han Xiao"><meta property="og:url" content="https://hanxiao.io/2020/09/21/Numpy-Tricks-and-A-Strong-Baseline-for-Vector-Index/"><meta property="og:image" content="https://hanxiao.io/2020/09/21/Numpy-Tricks-and-A-Strong-Baseline-for-Vector-Index//banner.png"><meta property="og:type" content="article"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" href="/wechaticon.png"><link rel="alternate" type="application/rss+xml" title="Han Xiao Tech Blog - Neural Search, AI, Engineering" href="https://hanxiao.io/atom.xml"><link rel="preload" href="/css/apollo.css" as="style" onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel="stylesheet" href="/css/apollo.css"></noscript><script>!function(n){"use strict";n.loadCSS||(n.loadCSS=function(){});var o=loadCSS.relpreload={};if(o.support=function(){var e;try{e=n.document.createElement("link").relList.supports("preload")}catch(t){e=!1}return function(){return e}}(),o.bindMediaToggle=function(t){var e=t.media||"all";function a(){t.media=e}t.addEventListener?t.addEventListener("load",a):t.attachEvent&&t.attachEvent("onload",a),setTimeout(function(){t.rel="stylesheet",t.media="only x"}),setTimeout(a,3e3)},o.poly=function(){if(!o.support())for(var t=n.document.getElementsByTagName("link"),e=0;e<t.length;e++){var a=t[e];"preload"!==a.rel||"style"!==a.getAttribute("as")||a.getAttribute("data-loadcss")||(a.setAttribute("data-loadcss",!0),o.bindMediaToggle(a))}},!o.support()){o.poly();var t=n.setInterval(o.poly,500);n.addEventListener?n.addEventListener("load",function(){o.poly(),n.clearInterval(t)}):n.attachEvent&&n.attachEvent("onload",function(){o.poly(),n.clearInterval(t)})}"undefined"!=typeof exports?exports.loadCSS=loadCSS:n.loadCSS=loadCSS}("undefined"!=typeof global?global:this)</script><link rel="search" type="application/opensearchdescription+xml" href="https://hanxiao.io/atom.xml" title="Han Xiao Tech Blog - Neural Search, AI, Engineering"></head><body><div class="reading-progress-bar"></div><div class="wrap"><header><a href="/" class="logo-link"><img src="/wechaticon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/subscribe/" target="_self" class="nav-list-link">SUBSCRIBE</a></li><li class="nav-list-item icon_item"><a href="https://www.linkedin.com/in/hxiao87" target="_blank" class="nav-list-link"><img src="/linkedin.svg" alt="linkedin" class="flag-icon"></a></li><li class="nav-list-item icon_item"><a href="https://twitter.com/hxiao" target="_blank" class="nav-list-link"><img src="/twitter.svg" alt="twitter" class="flag-icon"></a></li><li class="nav-list-item icon_item"><a href="https://github.com/hanxiao" target="_blank" class="nav-list-link"><img src="/github.svg" alt="github" class="flag-icon"></a></li><li class="nav-list-item icon_item"><a href="https://www.youtube.com/c/jina-ai" target="_blank" class="nav-list-link"><img src="/youtube.svg" alt="youtube" class="flag-icon"></a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Numpy Tricks and A Strong Baseline for Vector Index</h1><div class="post-info">Sep 21, 2020 by &nbsp;&nbsp;&nbsp;<img src="/myavatar.png" alt="logo" width="18px" height="18px" style="vertical-align:sub">&nbsp;<a href="https://www.linkedin.com/company/jinaai" target="_blank" class="nav-list-link">Han Xiao - Founder & CEO @ Jina AI üëê<i style="font-size:smaller"> We are hiring!</i></a></div><div class="post-info"><div class="read-time">‚ó∑&nbsp;&nbsp;&nbsp; 10 min read</div></div><div class="post-content"><p>I like simple things and not dependencies. When it comes to architecting <a href="https://github.com/jina-ai/jina" target="_blank" rel="noopener">Jina</a>, I keep its core dependencies as simple as <code>numpy</code>, <code>pyzmq</code>, <code>protobuf</code>, and <code>grpcio</code> while still delivering full-stack functionalities. This allows Jina developers &amp; users to bootstrap an end-to-end neural search system without extra packages quickly.</p><a id="more"></a><p>On the vector indexing and querying part, Jina has implemented a baseline vector indexer called <code>NumpyIndexer</code>, a vector indexer that is purely based on <code>numpy</code>. The implementation pretty straightforward: it writes vectors directly to the disk and queries nearest neighbors via dot product. It is simple, requires no extra dependencies, and the performance was reasonable on small data. As the default vector indexer, we have been using it since day one when showcasing quick demos, toy examples, and tutorials.</p><p>Recently, <a href="https://github.com/jina-ai/jina/issues/929" target="_blank" rel="noopener">this community issue</a> has raised my attention. I realize there is a space of improvement, even for this baseline indexer. In the end, I manage to improve the index and query speed by 1.6x and 2.8x while keeping the memory footprint constant (i.e., invariant to the size of the index data). This blog post summarizes the tricks I used.</p><img src="/2020/09/21/Numpy-Tricks-and-A-Strong-Baseline-for-Vector-Index/84215697.png" title="Jina v0.5.6 significantly reduces the memory footprint"><h4><span id="table-of-contents">Table of Contents</span></h4><ul><li><a href="#the-scalability-problem">The Scalability Problem</a></li><li><a href="#numpymemmap-instead-of-numpyfrombuffer">numpy.memmap Instead of numpy.frombuffer</a></li><li><a href="#batching-with-care">Batching with Care</a><ul><li><a href="#lifecycle-of-memmap">Lifecycle of memmap</a></li><li><a href="#zero-copy-slicing">Zero-copy slicing</a></li></ul></li><li><a href="#memory-efficient-euclidean-and-cosine">Memory-efficient Euclidean and Cosine</a></li><li><a href="#removing-gzip-compression">Removing gzip compression</a></li><li><a href="#summary">Summary</a></li></ul><h2><span id="the-scalability-problem">The Scalability Problem</span></h2><p>As <a href="https://github.com/jina-ai/jina/issues/929" target="_blank" rel="noopener">the issue</a> points out, <code>NumpyIndexer</code> faces a severe scalability problem: in the query time, it tries to load all data into memory, making it (and all inheritances) unusable on big data. If the user has 8GB physical memory on its laptop, then the biggest index size that <code>NumpyIndexer</code> can support is around 8GB, which is 7 digits embeddings at most. Having this in mind, my solution is built on <code>memmap</code> naturally.</p><h2><span id="numpymemmap-instead-of-numpyfrombuffer">numpy.memmap Instead of numpy.frombuffer</span></h2><p>Replacing the full-read to <a href="https://numpy.org/doc/stable/reference/generated/numpy.memmap.html" target="_blank" rel="noopener"><code>numpy.memmap</code></a> is trivial. Thanks to the ndarray format Jina followed in the index time; the only necessary work is changing how <code>raw_ndarray</code> is built at the query time.</p><table><thead><tr><th>Before</th><th>After</th></tr></thead><tbody><tr><td><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> gzip.open(self.index_abspath, <span class="string">'rb'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    self.raw_ndarray = np.frombuffer(fp.read(), dtype=self.dtype).reshape([<span class="number">-1</span>, self.num_dim])</span><br></pre></td></tr></table></figure></td><td><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.raw_ndarray = np.memmap(self.index_abspath, dtype=self.dtype, mode=<span class="string">'r'</span>, shape=(self.size, self.num_dim))</span><br></pre></td></tr></table></figure></td></tr></tbody></table><p>This reduces loading time basically to zero, as <code>ndarray</code> is now just a pointer to the virtual memory, it can be built in no time. The data behind this pointer is loaded on demand. Simple ops such as <code>ndarray[1]</code>, <code>ndarray.shape</code> won‚Äôt trigger the full-read.</p><h2><span id="batching-with-care">Batching with Care</span></h2><p>To query nearest neighbors <code>NumpyIndexer</code> computes dot products between query vectors and <em>all</em> indexed vectors row-wise. That means it anyway has to scan over the full data.If we stop here and don‚Äôt optimize any further, then the memory consumption is back to the high watermark after the first scan. The correct way is adding ‚Äúbatching‚Äù to all computations, e.g. dot products, cosine distance. Jina has provided a powerful <code>@batching</code> decorator from day one, and here is how to use it:</p><table><thead><tr><th>Before</th><th>After</th></tr></thead><tbody><tr><td><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="keyword">elif</span> self.metric == <span class="string">'euclidean'</span>:</span><br><span class="line">   dist = _euclidean(queries, self.raw_ndarray)</span><br></pre></td></tr></table></figure></td><td><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> self.metric == <span class="string">'euclidean'</span>:</span><br><span class="line">   dist = _batch_euclidean(queries, self.raw_ndarray)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="meta">@batching(merge_over_axis=1, slice_on=2)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_batch_euclidean</span><span class="params">(self, raw_A, raw_B)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> _euclidean(raw_A, raw_B)</span><br></pre></td></tr></table></figure></td></tr></tbody></table><h4><span id="lifecycle-of-memmap">Lifecycle of memmap</span></h4><p>This decorator will slice <code>raw_B</code> into batches and compute <code>_euclidean</code> one by one. All partial results are then merged before returning. However, it didn‚Äôt work as expected. Below is the memory footprint when running <code>100 x 10,000</code> query against <code>10,000 x 10,000</code> indexed vectors:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Line #    Mem usage    Increment   Line Contents</span><br><span class="line">================================================</span><br><span class="line">    27     62.1 MiB     62.1 MiB   @profile</span><br><span class="line">    28                             def query():</span><br><span class="line">    29     85.0 MiB      7.7 MiB       q = [np.random.random([num_query, num_dim]) for j in range(3)]</span><br><span class="line">    30     85.0 MiB      0.0 MiB       with TimeContext(&apos;query&apos;):</span><br><span class="line">    31     85.0 MiB      0.1 MiB           with NumpyIndexer.load(&apos;a.bin&apos;) as ni:</span><br><span class="line">    32   1172.8 MiB   1087.8 MiB               ni.query(q[0], top_k=10)</span><br><span class="line">    33                                         # query again and see if memory increasing further</span><br><span class="line">    34   1180.6 MiB      7.8 MiB               ni.query(q[1], top_k=10)</span><br><span class="line">    35                                         # query again and see if memory increasing further</span><br><span class="line">    36   1195.1 MiB     14.5 MiB               ni.query(q[2], top_k=10)</span><br></pre></td></tr></table></figure><p>One can observe that all data is loaded after the first <code>query</code>. Though each time <code>batch_euclidean</code> only works with partial data, the program eventually loads all data into the cache buffer. This buffer (as part of the virtual memory) is controlled on the OS level. <a href="https://github.com/numpy/numpy/issues/7732#issuecomment-225442996" target="_blank" rel="noopener">There</a> <a href="http://numpy-discussion.10968.n7.nabble.com/How-to-limit-the-numpy-memmap-s-RAM-usage-tp29078p29081.html" target="_blank" rel="noopener">are</a> <a href="https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once" target="_blank" rel="noopener">arguments</a> that claim this buffer will be released by OS automatically when the memory is intense. I decided to manually release this buffer by restricting the lifecycle of <code>memmap</code> inside the <code>batching</code> for-loop.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">data = np.memmap(self.index_abspath, dtype=self.dtype, mode=<span class="string">'r'</span>, shape=(self.size, self.num_dim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> slice <span class="keyword">in</span> batch_iterator(data[:total_size], b_size, split_over_axis, yield_slice=yield_slice):</span><br><span class="line">    <span class="comment"># make a new memmap</span></span><br><span class="line">    new_memmap = np.memmap(data.filename, dtype=data.dtype, mode=<span class="string">'r'</span>, shape=data.shape)</span><br><span class="line">    p_data = new_memmap[slice]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># do memory-intensive computation</span></span><br><span class="line">    _euclidean(q, p_data)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># close the memmap mannually</span></span><br><span class="line">    <span class="keyword">del</span> new_memmap</span><br></pre></td></tr></table></figure><h4><span id="zero-copy-slicing">Zero-copy slicing</span></h4><p>When generating <code>slice</code>, the previous Jina uses <a href="https://docs.scipy.org/doc//numpy/reference/generated/numpy.take.html" target="_blank" rel="noopener"><code>numpy.take</code></a>. One gotcha of <code>numpy.take</code> is it copies the data instead of slicing in-place, costing extra memory. One can use Python built-in <code>slice</code> function to implement zero-copy version of <code>numpy.take</code> as follows:</p><table><thead><tr><th><code>numpy.take</code></th><th>built-in <code>slice</code> (zero-copy)</th></tr></thead><tbody><tr><td><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.take(data, range(start, end), axis, mode=<span class="string">'clip'</span>)</span><br></pre></td></tr></table></figure></td><td><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_d = data.ndim</span><br><span class="line">sl = [slice(<span class="literal">None</span>)] * _d</span><br><span class="line">sl[axis] = slice(start, end)</span><br><span class="line">data[tuple(sl)]</span><br></pre></td></tr></table></figure></td></tr></tbody></table><h2><span id="memory-efficient-euclidean-and-cosine">Memory-efficient Euclidean and Cosine</span></h2><p>When computing Euclidean and Cosine distance row-wise on two matrices, one can simply use the broadcast feature of <code>ndarray</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numpy.sqrt(((A[:,:,<span class="literal">None</span>]-B[:,:,<span class="literal">None</span>].T)**<span class="number">2</span>).sum(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>However, <code>[:,:,None] - [:,:,None].T</code> creates a <code>(num_query, num_dim, num_data)</code> matrix in memory and this method can quickly become infeasible on big data. The space complexity is $O(N^3)$. In Jina, we use a more memory-efficient way to compute Euclidean distance:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_ext_A</span><span class="params">(A)</span>:</span></span><br><span class="line">    nA, dim = A.shape</span><br><span class="line">    A_ext = _get_ones(nA, dim * <span class="number">3</span>)</span><br><span class="line">    A_ext[:, dim:<span class="number">2</span> * dim] = A</span><br><span class="line">    A_ext[:, <span class="number">2</span> * dim:] = A ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> A_ext</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_ext_B</span><span class="params">(B)</span>:</span></span><br><span class="line">    nB, dim = B.shape</span><br><span class="line">    B_ext = _get_ones(dim * <span class="number">3</span>, nB)</span><br><span class="line">    B_ext[:dim] = (B ** <span class="number">2</span>).T</span><br><span class="line">    B_ext[dim:<span class="number">2</span> * dim] = <span class="number">-2.0</span> * B.T</span><br><span class="line">    <span class="keyword">return</span> B_ext</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_euclidean</span><span class="params">(A_ext, B_ext)</span>:</span></span><br><span class="line">    sqdist = A_ext.dot(B_ext).clip(min=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sqrt(sqdist)</span><br></pre></td></tr></table></figure><p>The idea is simply doing $(a-b)^2 = a^2 -2ab +b^2$ explicitly: first prefilling the values of $a^2$, $b^2$, $a$, $-2b$, and then computing $-a \times 2b$ part. This reduces the memory to <code>(num_query*3, num_dim) + (num_data*3, num_dim)</code> and the overall space complexity to $O(N^2)$.</p><p>Notice that when computing <code>_euclidean</code> with the <code>@batching</code> decorator, <code>A</code> is the query matrix and remains constant across all batching iterations. Hence, <code>A_ext</code> can be precomputed before the batching loop to avoid unnecessary computation.</p><p>Finally, the Cosine distance is nothing more but the ‚Äúnormed‚Äù Euclidean distance. It can be simply built on top of <code>_euclidean</code> function by feeding <code>_norm(A)</code> and <code>_norm(B)</code> to it:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_norm</span><span class="params">(A)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> A / np.linalg.norm(A, ord=<span class="number">2</span>, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2><span id="removing-gzip-compression">Removing gzip compression</span></h2><p>Jina uses <code>gzip</code> to store the vector data. This has been an old feature since the day one. However, this feature does not play well with <code>memmap</code> because the decompression will have to load all data into memory and this is against the idea of on-demand computing.</p><p>While removing <code>gzip</code> I found it didn‚Äôt add many benefits in the first place. The table below summarizes with/without <code>gzip</code> compression on <code>v0.5.5</code> (this version does not include any improvement mentioned abov). One can see that the index time increases 15x, whereas results in only 40MB space-saving.</p><table><thead><tr><th><code>v0.5.5</code> Mode</th><th style="text-align:right">Time (second)</th><th style="text-align:right">On-disk Space (MB)</th></tr></thead><tbody><tr><td>Index</td><td style="text-align:right">2.13</td><td style="text-align:right">800</td></tr><tr><td>Index (compressed)</td><td style="text-align:right">29.36</td><td style="text-align:right">759</td></tr><tr><td>Query</td><td style="text-align:right">4.56</td><td style="text-align:right"></td></tr><tr><td>Query (compressed)</td><td style="text-align:right">5.52</td></tr></tbody></table><p>The script for benchmarking <a href="https://gist.github.com/hanxiao/43cad33b60cadd34f45236993689e5d9" target="_blank" rel="noopener">can be found here</a>.</p><h2><span id="summary">Summary</span></h2><p>Putting everything together, now we get a pretty good baseline vector indexer that uses constant memory. We have rolled out this improved <code>NumpyIndexer</code> in <code>v0.5.6</code> patch and all examples, tutorials, and <code>jina hello-world</code> can immediately enjoy this improvement.</p><table><thead><tr><th>Version</th><th style="text-align:right">Time</th><th style="text-align:right"></th><th style="text-align:right">Memory</th><th style="text-align:right"></th></tr></thead><tbody><tr><td></td><td style="text-align:right">Index</td><td style="text-align:right">Query</td><td style="text-align:right">Index</td><td style="text-align:right">Query</td></tr><tr><td><code>v0.5.6</code></td><td style="text-align:right">1.34s</td><td style="text-align:right">1.61s</td><td style="text-align:right">7.3MB</td><td style="text-align:right">273.6MB</td></tr><tr><td><code>v0.5.5</code></td><td style="text-align:right">2.14s</td><td style="text-align:right">4.56s</td><td style="text-align:right">11.5MB</td><td style="text-align:right">910.2MB</td></tr><tr><td><code>v0.5.5</code> (compressed)</td><td style="text-align:right">29.36s</td><td style="text-align:right">5.52s</td><td style="text-align:right">11.4MB</td><td style="text-align:right">902.4MB</td></tr></tbody></table><p>To me the use of <code>memmap</code> is the most inspiring part. It makes me think a lot. This function and <code>memoryview()</code> can be used in Jina more extensively to improve its scalability further.</p></div></article></div></main><footer><div class="paginator"><a href="/2020/08/28/What-s-New-in-Jina-v0-5/" class="next">New Features in Jina...&nbsp;&nbsp;‚ùØ&nbsp;</a></div><div id="gitment_thread"></div><div class="copyright"><p>¬© 2017 - 2020 <a href="https://hanxiao.io">Han Xiao</a>. Opinions are solely my own. <img src="/by-nc-sa.svg" alt="Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License." class="image"></p></div></footer></div><link rel="stylesheet" href="/css/post/katex.min.css"><link rel="stylesheet" href="/css/post/gitment.css"><script src="/js/katex.min.js"></script><script src="/js/auto-render.min.js"></script><script>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})</script><script src="/js/gitment.browser.js"></script><script src="/js/gitment.loader.js"></script><script src="/js/jquery-3.4.1.min.js"></script><script src="/js/reading_progress.min.js"></script><script async src="https://www.google-analytics.com/analytics.js"></script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-52114253-1","auto"),ga("send","pageview")</script></body></html>