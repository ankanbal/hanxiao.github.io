<!DOCTYPE html><html lang="en"><style>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,header,main{display:block}a{background-color:transparent}h1{font-size:2em;margin:.67em 0}img{border:0}body,html{width:100%;height:100%}body{margin:0;color:#34495e;font-size:18px;line-height:1.6;background-color:#fff;font-family:sourcesanspro,'Helvetica Neue',Arial,sans-serif}ul.nav{margin:0;padding:0;list-style-type:none}ul{margin:1rem 0}a{color:#2c3e50;text-decoration:none}.flag-icon{height:25px;width:25px;display:inline;border-radius:50%;vertical-align:sub}.icon_item{padding-left:5px!important;padding-right:5px!important}header{min-height:60px}header .logo-link{float:left}header .nav{float:right;left:80px}header .logo-link img{height:60px}header .nav-list-item{display:inline-block;padding:19px 10px}header .nav-list-item a{line-height:1.4}@media screen and (max-width:900px){header .nav-list-item a{font-size:15px}}@media screen and (min-width:900px){header .nav-list-item a{font-size:18px}}.post{padding-top:1em}.post-block .post-title{margin:.65em 0;color:#2c3e50;font-size:1.5em}.post-block .post-info{color:#7f8c8d}.post-block .post-info .read-time{text-align:right}.post-content h2,.post-content h4{position:relative;margin:1em 0}.post-content h2 :before,.post-content h4 :before{content:"#";color:#42b983;position:absolute;left:-.7em;top:-4px;font-size:1.2em;font-weight:700}.post-content h4 :before{content:">"}.post-content h2{font-size:22px}.post-content h4{font-size:18px}.post-content a{color:#42b983;word-break:break-all}main.container{margin:2em 10px}@media screen and (min-width:900px){.wrap{width:900px;margin:0 auto}header{padding:20px 60px}}@media screen and (max-width:900px){.wrap{width:100%}header{min-height:50px;padding:2px 2px;position:fixed;z-index:10000;border-radius:15px;left:50%;-webkit-transform:translateX(-50%);transform:translateX(-50%);width:-webkit-fit-content;width:-moz-fit-content;width:fit-content}header a.logo-link,header ul.nav.nav-list{float:none;display:inline;text-align:center}header li.nav-list-item{padding:10px 5px}header .logo-link img{height:20px;vertical-align:sub}header .flag-icon{height:20px;width:20px}header{background-color:rgba(255,255,255,.9)}@supports ((-webkit-backdrop-filter:blur(2em)) or (backdrop-filter:blur(2em))){header{background-color:rgba(255,255,255,.3);-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px)}}main.container{padding-top:2em}main.container{margin:0 20px}.post-content h2,.post-content h4{max-width:300px;left:15px}}@font-face{font-family:sourcesanspro;src:url(/font/sourcesanspro.woff2) format("woff2"),url(/font/sourcesanspro.woff) format("woff");font-weight:400;font-style:normal}</style><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title>4 Sequence Encoding Blocks You Must Know Besides RNN/LSTM in Tensorflow · Han Xiao Tech Blog - Neural Search, AI, Engineering</title><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@hxiao"><meta name="twitter:creator" content="@hxiao"><meta name="description" content="Understanding human language is a challenging task for computers, as they were originally designed for crunching numbers. To let computers comprehend ... · Han Xiao"><meta property="og:title" content="4 Sequence Encoding Blocks You Must Know Besides RNN/LSTM in Tensorflow · Han Xiao Tech Blog - Neural Search, AI, Engineering"><meta property="og:description" content="Understanding human language is a challenging task for computers, as they were originally designed for crunching numbers. To let computers comprehend ... · Han Xiao"><meta property="og:url" content="https://hanxiao.io/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/"><meta property="og:image" content="https://hanxiao.io/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow//f3020ac6.png"><meta property="og:type" content="article"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" href="/wechaticon.png"><link rel="alternate" type="application/rss+xml" title="Han Xiao Tech Blog - Neural Search, AI, Engineering" href="https://hanxiao.io/atom.xml"><link rel="preload" href="/css/apollo.css" as="style" onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel="stylesheet" href="/css/apollo.css"></noscript><script>!function(n){"use strict";n.loadCSS||(n.loadCSS=function(){});var o=loadCSS.relpreload={};if(o.support=function(){var e;try{e=n.document.createElement("link").relList.supports("preload")}catch(t){e=!1}return function(){return e}}(),o.bindMediaToggle=function(t){var e=t.media||"all";function a(){t.media=e}t.addEventListener?t.addEventListener("load",a):t.attachEvent&&t.attachEvent("onload",a),setTimeout(function(){t.rel="stylesheet",t.media="only x"}),setTimeout(a,3e3)},o.poly=function(){if(!o.support())for(var t=n.document.getElementsByTagName("link"),e=0;e<t.length;e++){var a=t[e];"preload"!==a.rel||"style"!==a.getAttribute("as")||a.getAttribute("data-loadcss")||(a.setAttribute("data-loadcss",!0),o.bindMediaToggle(a))}},!o.support()){o.poly();var t=n.setInterval(o.poly,500);n.addEventListener?n.addEventListener("load",function(){o.poly(),n.clearInterval(t)}):n.attachEvent&&n.attachEvent("onload",function(){o.poly(),n.clearInterval(t)})}"undefined"!=typeof exports?exports.loadCSS=loadCSS:n.loadCSS=loadCSS}("undefined"!=typeof global?global:this)</script><link rel="search" type="application/opensearchdescription+xml" href="https://hanxiao.io/atom.xml" title="Han Xiao Tech Blog - Neural Search, AI, Engineering"></head><body><div class="reading-progress-bar"></div><div class="wrap"><header><a href="/" class="logo-link"><img src="/wechaticon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/subscribe/" target="_self" class="nav-list-link">SUBSCRIBE</a></li><li class="nav-list-item icon_item"><a href="https://www.linkedin.com/in/hxiao87" target="_blank" class="nav-list-link"><img src="/linkedin.svg" alt="linkedin" class="flag-icon"></a></li><li class="nav-list-item icon_item"><a href="https://twitter.com/hxiao" target="_blank" class="nav-list-link"><img src="/twitter.svg" alt="twitter" class="flag-icon"></a></li><li class="nav-list-item icon_item"><a href="https://github.com/hanxiao" target="_blank" class="nav-list-link"><img src="/github.svg" alt="github" class="flag-icon"></a></li><li class="nav-list-item icon_item"><a href="https://www.youtube.com/c/jina-ai" target="_blank" class="nav-list-link"><img src="/youtube.svg" alt="youtube" class="flag-icon"></a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">4 Sequence Encoding Blocks You Must Know Besides RNN/LSTM in Tensorflow</h1><div class="post-info">Jun 24, 2018 by &nbsp;&nbsp;&nbsp;<img src="/myavatar.png" alt="logo" width="18px" height="18px" style="vertical-align:sub">&nbsp;<a href="/about" target="_blank" class="nav-list-link">Han Xiao - <i>ex</i> Engineering Lead @ Tencent AI Lab</a></div><div class="post-info"><div class="read-time">◷&nbsp;&nbsp;&nbsp; 17 min read</div></div><div class="post-content"><h2><span id="background">Background</span></h2><p>Understanding human language is a challenging task for computers, as they were originally designed for crunching numbers. To let computers comprehend text as humans do, one needs to encode the complexities and nuances of natural language into numbers. For many years, <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" target="_blank" rel="noopener">recurrent neural networks (RNN)</a> or <a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_blank" rel="noopener">long-short term memory (LSTM)</a> was the way to solve sequence encoding problem. They have indeed accomplished amazing results in many applications, e.g. machine translation and voice recognition. As for me, they were the first solution that comes to my mind when facing an NLP problem. You can find my previous posts about RNN/LSTM in <a href="/2018/04/21/Teach-Machine-to-Comprehend-Text-and-Answer-Question-with-Tensorflow/" title="here">here</a>, <a href="/2018/01/10/Build-Cross-Lingual-End-to-End-Product-Search-using-Tensorflow/" title="here">here</a>, <a href="/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/" title="and here">and here</a>.</p><p>Now at Tencent AI Lab, however, I’m <em>sunsetting</em> RNN/LSTM in my team. Why? Because they are <strong>computationally expensive</strong>, aka slow! The recursive nature<a id="more"></a> requires it to maintain a hidden state of the entire past that prevents parallel computation within a sequence. Also, don’t forget that sequence encoding is often just a low-level subtask of our ultimate goal (e.g. translation, question-answering). The upper layers are waiting for the encodes so that they can carry on more interesting tasks. Thus, it is not worth spending so much time on the encoding task itself.</p><p>All in all, we need our sequence encoding to be fast. A faster sequence encoder boosts the development cycle of our team, enabling us to find the optimal model by exploring more architectures and hyperparameters, which is crucial for bringing AI into production.</p><p>In this article, I will introduce four sequence encoding blocks as a replacement of RNN/LSTM. I’d like to call them “block”, as they all consist of multiple layers or even other blocks. They can be further nested, duplicated or joined to compose a more power super-block, improving a network’s capacity and expressivity.</p><p>The code mentioned in this post <a href="https://github.com/hanxiao/encoding-blocks" target="_blank" rel="noopener">can be found on my Github</a>. Readers are welcome to contribute or give suggestions.</p><h4><span id="table-of-content">Table of Content</span></h4><ul><li><a href="#what-is-a-sequence-encoding-block">What is a Sequence Encoding Block</a></li><li><a href="#pooling-block">Pooling Block</a><ul><li><a href="#average-pooling-and-max-pooling">Average Pooling and Max Pooling</a></li><li><a href="#hierarchical-pooling">Hierarchical Pooling</a></li><li><a href="#attentive-pooling">Attentive Pooling</a></li><li><a href="#final-touch">Final Touch</a></li></ul></li><li><a href="#cnn-block">CNN Block</a><ul><li><a href="#causal-convolutions">Causal Convolutions</a></li></ul></li><li><a href="#multi-resolution-cnn-block">Multi-Resolution CNN Block</a></li><li><a href="#multi-head-multi-resolution-cnn-block">Multi-Head Multi-Resolution CNN Block</a><ul><li><a href="#implementation-trick">Implementation Trick</a></li></ul></li><li><a href="#summary">Summary</a></li></ul><h2><span id="what-is-a-sequence-encoding-block">What is a Sequence Encoding Block</span></h2><p>As you may find people referring the encoding task differently, such as fusing, composing and aggregating. I’d like to first clarify the definition of an encoding block, so we are on the same page.</p><p>Considering a text sequence (e.g. a sentence, a paragraph, a document) represented as $\mathbf{s}:=\{w_1, w_2, \ldots, w_L\,|\,w_l \in \mathcal{V}\}$, where $w$ denotes a word from the vocabulary $\mathcal{V}$ and $L$ denotes the length of the sequence. Let $\mathbf{M}$ be a word embedding matrix of size $|\mathcal{V}|\times D$, each row represents a word in $\mathbb{R}^D$. The word embedding matrix $\mathbf{M}$ can be obtained with <a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">GloVe</a> or <a href="https://en.wikipedia.org/wiki/Word2vec" target="_blank" rel="noopener">Word2Vec</a>, or be initialized randomly and learned from scratch. Either way, one can now represent $\mathbf{s}$ as a $L\times D$ matrix via <code>tf.embedding_lookup</code> on $\mathbf{M}$. The goal of a sequence encoding block is to find a function $\mathbb{R}^{L\times D}\rightarrow\mathbb{R}^{D^\prime}$, which encodes information of a variable length sentence into a fixed-length vector representation. The $D^\prime$-dimensional output vector is then fed to the upper layers for the final task. The next figure illustrates this idea.</p><img src="/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/fc9bf65e.png"><h2><span id="pooling-block">Pooling Block</span></h2><h4><span id="average-pooling-and-max-pooling">Average Pooling and Max Pooling</span></h4><p>Let’s start with the simplest ones: reducing the dimension $L$ to 1 by taking the average/maximum over each of the $D$ dimensions. They can be (almost) easily implemented <code>tf.reduce_mean</code> and <code>tf.reduce_max</code>, respectively. I say <em>almost</em>, as one has to be careful with the padded symbols when dealing with a batch of sequences of different lengths. Those paddings should <em>never</em> be involved in the calculation. Tiny details, yet people often forget about them.</p><p>These two strategies are also called <em>average pooling</em> and <em>max pooling</em>, respectively. Both of them make perfect sense intuitively: in average pooling the meaning of a sentence is represented by all words contained; whereas in max pooling the meaning is represented by a small number of keywords and their salient features only.</p><h4><span id="hierarchical-pooling">Hierarchical Pooling</span></h4><p>To keep the best part of two sides, one can combine these two strategies together: via concatenating, i.e. <code>tf.concat</code>, or <em>hierarchical pooling</em>. In hierarchical pooling, we first define a small sliding window. The window moves across the dimension $L$. Then we do <code>reduce_mean</code> at every move and collect the result, finally do <code>reduce_max</code> on all results. Here I show a Tensorflow implementation of the hierarchical pooling strategy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reshape_seqs</span><span class="params">(x, avg_window_size=<span class="number">3</span>, **kwargs)</span>:</span></span><br><span class="line">    B = tf.shape(x)[<span class="number">0</span>]</span><br><span class="line">    L = tf.cast(tf.shape(x)[<span class="number">1</span>], tf.float32)</span><br><span class="line">    D = x.get_shape().as_list()[<span class="number">-1</span>]</span><br><span class="line">    b = tf.transpose(x, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">    extra_pads = tf.cast(tf.ceil(L / avg_window_size) * avg_window_size - L, tf.int32)</span><br><span class="line">    c = tf.pad(b, tf.concat([tf.zeros([<span class="number">2</span>, <span class="number">2</span>], dtype=tf.int32), [[<span class="number">0</span>, extra_pads]]], axis=<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">return</span> tf.reshape(c, [B, D, avg_window_size, <span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># avg pooling with mask, be careful with all zero mask</span></span><br><span class="line">d = tf.reduce_sum(reshape_seqs(seqs, **kwargs), axis=<span class="number">2</span>) / \</span><br><span class="line">    tf.reduce_sum(reshape_seqs(tf.expand_dims(mask + <span class="number">1e-10</span>, axis=<span class="number">-1</span>), **kwargs), axis=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># max pooling</span></span><br><span class="line">pooled = tf.reduce_max(d, axis=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>The trick here is to reshape the batch sequence in advance from the shape <code>[B,L,D]</code> to <code>[B,D,W,L&#39;/W]</code>, where <code>W</code> is the size of the sliding window, and <code>L&#39;</code> is the length with <em>extra paddings</em> so that it can be divided by <code>W</code>. As a consequence, one can simply apply average pooling on the third dimension followed by a max pooling on the last dimension. Just be careful with padded symbols and the mask.</p><p>Comparing to max and average pooling, hierarchical pooling captures more local spatial information because of the preservation of the word-order in each window. The size of the window is a trade-off between the max and average pooling. One may also relate it with <a href="https://arxiv.org/abs/1509.01626" target="_blank" rel="noopener">bag-of-n-grams described in this paper</a>.</p><h4><span id="attentive-pooling">Attentive Pooling</span></h4><p>We can also let the model automatically pick up the important words to represent a sentence. One solution is attentive pooling. Specifically, I introduce a $D$-dimensional global variable $\mathbf{q}$. We first multiply the sequence with $\mathbf{q}$ and get a “score” of length $L$. Each element of the score represents the importance of a word to the sequence. Finally, a weighted sum is conducted over all words according to the score. The figure below illustrates the attention procedure.</p><img src="/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/e191ea49.png"><p>The code is simple as follows:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">B = tf.shape(seqs)[<span class="number">0</span>]</span><br><span class="line">D = tf.shape(seqs)[<span class="number">-1</span>]</span><br><span class="line">query = tf.tile(get_var(<span class="string">'query'</span>, shape=[<span class="number">1</span>, D, <span class="number">1</span>]), [B, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">score = tf.nn.softmax(minus_mask(tf.matmul(seqs, query) / (D ** <span class="number">0.5</span>), mask), axis=<span class="number">1</span>)  <span class="comment"># [B,L,1]</span></span><br><span class="line">pooled = tf.reduce_sum(score * seqs, axis=<span class="number">1</span>)  <span class="comment"># [B, D]</span></span><br></pre></td></tr></table></figure><p></p><p>Again, the mask and padded symbols need to be taken care of before computing <code>softmax</code>. This can be done by subtracting a large number using <code>minus_mask</code>. Besides the scaled matrix multiplication mentioned above, there are other attention functions. Interested readers are recommended to read <a href="https://arxiv.org/abs/1711.07341" target="_blank" rel="noopener">Sect. 4.3 in this paper</a>.</p><h4><span id="final-touch">Final Touch</span></h4><p>As our first block, I don’t want to make it too complicated. So let’s finish it with some final touch. Here I add layer normalization and dropout to improve the robustness and generalization ability. Finally, a complete pooling block looks like the following:</p><img src="/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/761e3373.png"><p>As we shall see later, the pooling block is a cornerstone for more sophisticated encoding blocks.</p><h2><span id="cnn-block">CNN Block</span></h2><p>There has been excellent work with CNNs on NLP before, as in <a href="http://arxiv.org/abs/1103.0398" target="_blank" rel="noopener">“Natural Language Processing (almost) from Scratch”</a>. The basic idea is to use a 1-D kernel with a small width and slide it over the entire sequence. In some sense, this idea is very similar to the hierarchical pooling as I described in the last section. The major difference is that CNN’s kernel is not a simple average: it is parameterized and needs to be learned from the data. The code below describes how to apply convolution on a sequence:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">seq_dim = seqs.get_shape().as_list()[<span class="number">-1</span>]</span><br><span class="line">W = get_var(<span class="string">'conv_W'</span>, [kernel_width, seq_dim, num_filters])</span><br><span class="line">b = get_var(<span class="string">'conv_bias'</span>, [num_filters])</span><br><span class="line">conv = tf.nn.conv1d(seqs, W, stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>)</span><br><span class="line">output = tf.nn.relu(tf.nn.bias_add(conv, b))</span><br></pre></td></tr></table></figure><p></p><p>Note that the output has the shape <code>[L, D]</code> when using <code>padding=&#39;SAME&#39;</code>, meaning the full input sequence length $L$ is retained. To introduce some non-linearities, I add a bias to it and wrap it with a ReLU activation function. One may also understand it as a “gate”, keeping only the salient values for the output. Finally, the output is fed to a pooling block for transforming it into a fixed-length vector. The next figure illustrates this process:</p><img src="/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/06a2a68b.png"><h4><span id="causal-convolutions">Causal Convolutions</span></h4><p>One may argue that the above CNN block is <em>cheating</em> as it leverages the future information. To make it clear, assuming we have a block with <code>kernel_width=3</code> then the convoluted output of time $l$ depends on the input of $l-1$, $l$ and $l+1$. This is different than standard RNN/LSTM, where each step $l$ only depends on the past ($\leq l$) not on the future.</p><p>To make it more RNN/LSTM-like, I introduce <em>causal convolutions</em>, in which an output at time $l$ is convolved only with elements from time $l$ and earlier in the input. This can be simply done by padding <code>kernel_width -1</code> empty symbols to the left of the sequence. The next figure illustrates this idea vs. the standard convolution.</p><img src="/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/09bdc17c.png"><p>The code is simply as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">padding = (kernel_width - <span class="number">1</span>)</span><br><span class="line">padded_seq = tf.pad(seqs, [[<span class="number">0</span>, <span class="number">0</span>], [padding, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">output = tf.layers.conv1d(padded_seq, num_filters, kernel_width, activation=tf.nn.relu)</span><br></pre></td></tr></table></figure><p>Shifting the sequence to change time-dependency of the model is nothing new. It is essentially the same tweak as described in the <a href="https://www.cs.toronto.edu/~hinton/absps/waibelTDNN.pdf" target="_blank" rel="noopener">time delay neural network proposed nearly 30 years ago</a>.</p><h2><span id="multi-resolution-cnn-block">Multi-Resolution CNN Block</span></h2><p>The hyperparameter <code>kernel_width</code> controls the resolution of the CNN block. When applying it on a sequence, <code>kernel_width</code> corresponds to the context size $n$ of n-gram. An ideal kernel width is important but also task-dependent. As a workaround, one can simply employ multiple CNN blocks with different kernel widths and concatenate their outputs together. The code is presented below:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">outputs = [Pool_Block(seqs, mask, **kwargs)] <span class="keyword">if</span> highway <span class="keyword">else</span> []</span><br><span class="line"><span class="keyword">for</span> fw <span class="keyword">in</span> kernel_widths:</span><br><span class="line">    outputs.append(CNN_Block(seqs, mask, fw, scope=<span class="string">'cnn_%d'</span> % fw, **kwargs))</span><br><span class="line">output = tf.concat(outputs, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p></p><p>Note that I let the input bypass all CNN blocks when <code>highway=True</code>. This shortcut is particularly useful when training a very deep network, as the gradient can be propagated back to the low-level layers more easily through this highway. The next figure visualizes this block:</p><img src="/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/e0e8ffff.png"><p>One may also stack CNN blocks on top of each other to increase the context size. For instance, stacking 5 CNN blocks with kernel width of 3 results in an input field of 11 words, i.e., each output depends on 11 input words. Personally, I found such deep encoders are more difficult to train well in practice, and its improvement on NLP tasks is marginal. <a href="https://arxiv.org/abs/1611.02344" target="_blank" rel="noopener">But who am I to say that?</a></p><h2><span id="multi-head-multi-resolution-cnn-block">Multi-Head Multi-Resolution CNN Block</span></h2><p>The term “multi-head” comes from Google’s paper <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention is All You Need</a>, where it stacks multiple attention bocks for a single input and creates a powerful but fast-to-train model. The parameters of those attention blocks are <em>not</em> shared. The expectation is that each “head” can capture a unique aspect of the sequence, thus together they can improve the expressivity of the model.</p><p>Inspired by this idea, here I stack multiple MR-CNN blocks to build a multi-head multi-resolution CNN block. A fully expanded view of this block is depicted in the next figure. A global highway is probably not necessary here as each MR-CNN block already has its own highway.</p><img src="/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/91797b86.png"><h4><span id="implementation-trick">Implementation Trick</span></h4><p>While we keep stacking and nesting blocks into one, the number of hyperparameters keeps increasing. This also means the function identifier contains quite some arguments. Take a MH-MR-CNN block as an example,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">encode = MH_MR_CNN(seq, mask,</span><br><span class="line">                  num_heads=num_heads,</span><br><span class="line">                  kernel_sizes=kernel_sizes,</span><br><span class="line">                  num_units=args.fuse_num_units,</span><br><span class="line">                  dropout=ph_dropout_keep_prob,</span><br><span class="line">                  norm_by_layer=args.global_layer_norm,</span><br><span class="line">                  reduce=args.fuse_reduce,</span><br><span class="line">                  highway=args.fuse_highway)</span><br></pre></td></tr></table></figure><p>Note that most of the arguments are <em>not</em> for MH-MR-CNN except <code>num_heads</code>. Nonetheless, they still need to be carried all the way back to the deepest block, i.e. the pooling block. A naive way to implement this is writing all hyperparameters in all function identifiers, passing them via explicit function arguments. A better way is to use <code>**kwargs</code>, a Python feature which allows you to pass a variable number of arguments to a function. If a function can’t handle some of those arguments, then you can simply forward <code>**kwargs</code> to its sub-functions, etc. The code below demonstrates this trick:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Pool</span><span class="params">(seqs, mask, reduce=<span class="string">'concat_mean_max'</span>, norm_by_layer=False, dropout_keep=<span class="number">1.</span>, **kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CNN</span><span class="params">(seqs, mask, kernel_size, num_units=None, **kwargs)</span>:</span></span><br><span class="line">    <span class="comment"># ... POOL(seqs, mask, **kwargs) ...</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MR_CNN</span><span class="params">(seqs, mask, kernel_sizes=<span class="params">(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span>, highway=False, **kwargs)</span>:</span></span><br><span class="line">    <span class="comment"># ... CNN(seqs, mask, **kwargs)  ...</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MH_MR_CNN</span><span class="params">(seqs, mask, num_heads=<span class="number">3</span>, **kwargs)</span>:</span></span><br><span class="line">    <span class="comment"># ... MR_CNN(seqs, mask, **kwargs) ...</span></span><br></pre></td></tr></table></figure><p>As one can see, by using <code>**kwargs</code> each block only declares its own hyperparameters in the function identifier, resulting in a much cleaner and more readable code.</p><h2><span id="summary">Summary</span></h2><p>A sequence encoder is the cornerstone of many advanced AI applications, such as semantic search, question-answering, machine reading comprehension. Therefore, how to encode sequences good and fast is considered as a fundamental problem in the ML/NLP community. In this post, I have introduced four non-RNN encoding blocks which are extremely useful in practice. In some NLP task where long-term dependencies are not so important (e.g. classification), those non-RNN encoding blocks can perform competitively to recurrent alternatives and save you a lot of computational time.</p><p>The code mentioned in this post <a href="https://github.com/hanxiao/encoding-blocks" target="_blank" rel="noopener">can be found on my Github</a>.</p></div></article></div></main><footer><div class="paginator"><a href="/2018/08/25/谈情说-AI-访德中人工智能协会主席肖涵博士/" class="prev">&nbsp;❮&nbsp;&nbsp;德国人工智能对比中国人工智能：大学、产业...</a><a href="/2018/04/21/Teach-Machine-to-Comprehend-Text-and-Answer-Question-with-Tensorflow/" class="next">Teach Machine to Com...&nbsp;&nbsp;❯&nbsp;</a></div><div id="gitment_thread"></div><div class="copyright"><p>© 2017 - 2020 <a href="https://hanxiao.io">Han Xiao</a>. Opinions are solely my own. <img src="/by-nc-sa.svg" alt="Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License." class="image"></p></div></footer></div><link rel="stylesheet" href="/css/post/katex.min.css"><link rel="stylesheet" href="/css/post/gitment.css"><script src="/js/katex.min.js"></script><script src="/js/auto-render.min.js"></script><script>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})</script><script src="/js/gitment.browser.js"></script><script src="/js/gitment.loader.js"></script><script src="/js/jquery-3.4.1.min.js"></script><script src="/js/reading_progress.min.js"></script><script async src="https://www.google-analytics.com/analytics.js"></script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-52114253-1","auto"),ga("send","pageview")</script></body></html>